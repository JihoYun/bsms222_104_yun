---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

# 16 statistical models
# 16.1 poll aggregators

```{r}
library(tidyverse)
library(dslabs)
d <- 0.039
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p <- (d + 1) / 2

polls <- map_df(Ns, function(N) {
  x <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
  x_hat <- mean(x)
  se_hat <- sqrt(x_hat * (1 - x_hat) / N)
  list(estimate = 2 * x_hat - 1, 
    low = 2*(x_hat - 1.96*se_hat) - 1, 
    high = 2*(x_hat + 1.96*se_hat) - 1,
    sample_size = N)
}) %>% mutate(poll = seq_along(Ns))
```

```{r}
sum(polls$sample_size)
```

```{r}
d_hat <- polls %>% 
  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %>% 
  pull(avg)
```
# 16.1.1 poll data

```{r}
data(polls_us_election_2016)
```

```{r}
polls <- polls_us_election_2016 %>% 
  filter(state == "U.S." & enddate >= "2016-10-31" &
           (grade %in% c("A+","A","A-","B+") | is.na(grade)))
```

```{r}
polls <- polls %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```
```{r}
d_hat <- polls %>% 
  summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %>% 
  pull(d_hat)
```
```{r}
p_hat <- (d_hat+1)/2 
moe <- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))
moe
```

```{r}
polls %>%
  ggplot(aes(spread)) +
  geom_histogram(color="black", binwidth = .01)
```

# 16.1.2 pollster bias

```{r}
polls %>% group_by(pollster) %>% summarize(n())
```

```{r}
polls %>% group_by(pollster) %>% 
  filter(n() >= 6) %>%
  summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))
```

# 16.2 data-driven models

```{r}
one_poll_per_pollster <- polls %>% group_by(pollster) %>% 
  filter(enddate == max(enddate)) %>%
  ungroup()
```

```{r}
qplot(spread, data = one_poll_per_pollster, binwidth = 0.01)
```
```{r}
sd(one_poll_per_pollster$spread)
```
```{r}
results <- one_poll_per_pollster %>% 
  summarize(avg = mean(spread), 
            se = sd(spread) / sqrt(length(spread))) %>% 
  mutate(start = avg - 1.96 * se, 
         end = avg + 1.96 * se) 
round(results * 100, 1)
```

# 16.3 exercises

```{r}
library(dslabs)
data(heights)
x <- heights %>% filter(sex == "Male") %>%
  pull(height)
```

1. Mathematically speaking, x is our population. Using the urn analogy, we have an urn with the values of x in it. What are the average and standard deviation of our population?

```{r}
mean(x)
sd(x)
```

2.Call the population average computed above  μ  and the standard deviation  σ. Now take a sample of size 50, with replacement, and construct an estimate for  μ  and  σ .

```{r}
n <- 50
X <- sample(x, n, replace = TRUE)
mean(X)
```
3.What does the theory tell us about the sample average  ¯X  and how it is related to  μ ?

a.It is practically identical to  μ .
b.It is a random variable with expected value  μ  and standard error  σ/√N .
c.It is a random variable with expected value  μ  and standard error  σ .
d.Contains no information.

b

4.So how is this useful? We are going to use an oversimplified yet illustrative example. Suppose we want to know the average height of our male students, but we only get to measure 50 of the 708. We will use  ¯X  as our estimate. We know from the answer to exercise 3 that the standard estimate of our error  ¯X−μ  is  σ/√N . We want to compute this, but we don’t know  σ . Based on what is described in this section, show your estimate of  σ .

```{r}
sd(X)
```

5.Now that we have an estimate of  σ , let’s call our estimate  s . Construct a 95% confidence interval for  μ .

```{r}
mean(X) + c(-1, 1)*1.96 * sd(X) / sqrt(n)
```

6. Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include  μ ?

```{r}
mu <- mean(x)


B <- 10000
monte <- replicate(B, {
  X <- sample(x, n, replace=TRUE)
  interval <- mean(X) + c(-1,1)*1.96*sd(X)/sqrt(n)
  between(mu, interval[1], interval[2])
})
mean(monte)
```

7.In this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.

```{r}
data(polls_us_election_2016)
polls <- polls_us_election_2016 %>% 
  filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research",
                         "The Times-Picayune/Lucid") &
           enddate >= "2016-10-15" &
           state == "U.S.") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) 
```
We want to answer the question: is there a poll bias? Make a plot showing the spreads for each poll.


```{r}
polls %>% ggplot(aes(pollster, spread)) + 
  geom_boxplot()
```
8.The data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.

The urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call  d .
To answer the question “is there an urn model?”, we will model the observed data  Yi,j  in the following way:Yi,j=d+bi+εi,j with i=1,2  indexing the two pollsters,  bi  the bias for pollster  i  and  εij  poll to poll chance variability. We assume the  ε are independent from each other, have expected value  0  and standard deviation  σi  regardless of  j .
Which of the following best represents our question?

a.Is  εi,j  = 0?
b.How close are the  Yi,j  to  d ?
c.Is  b1≠b2 ?
d.Are  b1=0  and  b2=0  ?

c

9.In the right side of this model only  εi,j  is a random variable. The other two are constants. What is the expected value of  
Y1,j ?

d+b1

10. Suppose we define  ¯Y1  as the average of poll results from the first poll,  Y,1,…,Y1,N1  with  N1  the number of polls conducted by the first pollster:
```{r}
polls %>% 
  filter(pollster=="Rasmussen Reports/Pulse Opinion Research") %>% 
  summarize(N_1 = n())
```

What is the expected values  ¯Y1?

d+b1

11.What is the standard error of  Y1  ?

σ1/sqrt(N1)

12.Suppose we define  ¯Y2  as the average of poll results from the first poll,  Y2,1,…,Y2,N2  with  N2  the number of polls conducted by the first pollster. What is the expected value  ¯Y2 ?

d+b2

13.What is the standard error of  ¯Y2 ?

σ2/sqrt(N2)

14.Using what we learned by answering the questions above, what is the expected value of  ¯Y2−¯Y1 ?

b2-b1

15.Using what we learned by answering the questions above, what is the standard error of  ¯Y2−¯Y1 ?

sqrt(σ2^2/N2+σ1^2/N1)

16.The answer to the question above depends on  σ1  and  σ2 , which we don’t know. We learned that we can estimate these with the sample standard deviation. Write code that computes these two estimates.

```{r}
polls %>% group_by(pollster) %>%
  summarize(s = sd(spread))
```
17.What does the CLT tell us about the distribution of  ¯Y2−¯Y1 ?

a.Nothing because this is not the average of a sample.
b.Because the  Yij are approximately normal, so are the averages.
c.Note that  ¯Y2  and  ¯Y1  are sample averages, so if we assume  N2  and  N1 are large enough, each is approximately normal. The difference of normals is also normal.
d.The data are not 0 or 1, so CLT does not apply.

c

18.We have constructed a random variable that has expected value  b2−b1 , the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on  σ1  and  σ2 , but we can plug the sample standard deviations we computed above. We started off by asking: is  b2−b1  different from 0? Use all the information we have learned above to construct a 95% confidence interval for the difference  b2  and  b1 .

```{r}
X <- polls %>% group_by(pollster) %>% 
  summarize(avg = mean(spread), sd = sd(spread), N = n()) 
b2b1 <- X$avg[2] - X$avg[1]
se_hat <- with(X, sqrt(sd[2]^2/N[2] + sd[1]^2/N[1]))
b2b1 + c(-1,1)*1.96*se_hat
```
19. The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. Random variability does not seem to explain it. We can compute a p-value to relay the fact that chance does not explain it. What is the p-value?

```{r}
2*(1 - pnorm(b2b1/se_hat, 0, 1))
```
20.The statistic formed by dividing our estimate of  b2-b1  by its estimated standard error:

is called the t-statistic. Now notice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can actually construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect?

For this exercise, create a new table:

```{r}
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-15" &
           state == "U.S.") %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  ungroup()
```
Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.

```{r}
polls %>% group_by(pollster) %>%
  summarize(avg = mean(spread), sd = sd(spread)) 
```

# 16.4 bayesian statistics

# 16.4.1 bayes theorem

```{r}
prev <- 0.00025
N <- 100000
outcome <- sample(c("Disease","Healthy"), N, replace = TRUE, 
                  prob = c(prev, 1 - prev))
```

```{r}
N_D <- sum(outcome == "Disease")
N_D
#> [1] 23
N_H <- sum(outcome == "Healthy")
N_H
```

```{r}
accuracy <- 0.99
test <- vector("character", N)
test[outcome == "Disease"]  <- sample(c("+", "-"), N_D, replace = TRUE, 
                                    prob = c(accuracy, 1 - accuracy))
test[outcome == "Healthy"]  <- sample(c("-", "+"), N_H, replace = TRUE, 
                                    prob = c(accuracy, 1 - accuracy))
```

```{r}
table(outcome,test)
```

# 16.5.1 bayes in practice

The average player had an AVG of .275 and the standard deviation of the population of players was 0.027. So we can see already that .450 would be quite an anomaly since it is over six standard deviations away from the mean.

 If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential.


# 16.6 hierarchical models

The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example,  p . Then we see 20 random outcomes with success probability  p .

# 16.7 exercises

1.In 1999, in England, Sally Clark58 was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998. In both cases, she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500 and then calculating that the chance of two SIDS cases was 8,500  ×  8,500 ≈  73 million. Which of the following do you agree with? 

a.Sir Meadow assumed that the probability of the second son being affected by SIDS was independent of the first son being affected, thereby ignoring possible genetic causes. If genetics plays a role then: Pr(second case of SIDS|first case of SIDS)<Pr(first case of SIDS).
b.Nothing. The multiplication rule always applies in this way: Pr(A and B)=Pr(A)Pr(B).
c.Sir Meadow is an expert and we should trust his calculations.
d.Numbers don’t lie.

a

2.Let’s assume that there is in fact a genetic component to SIDS and the probability of Pr(second case of SIDS|first case of SIDS)=1/100, is much higher than 1 in 8,500. What is the probability of both of her sons dying of SIDS?

```{r}
(1/8500)*(1/100)
```

3.Many press reports stated that the expert claimed the probability of Sally Clark being innocent as 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written as the probability of a mother is a son-murdering psychopath given that two of her children are found dead with no evidence of physical harm. According to Bayes’ rule, what is this?
hierarchical models

4.Assume that the chance of a son-murdering psychopath finding a way to kill her children, without leaving evidence of physical harm, is: Pr(A|B)=.50 with A = two of her children are found dead with no evidence of physical harm and B = a mother is a son-murdering psychopath = 0.50. Assume that the rate of son-murdering psychopaths mothers is 1 in 1,000,000. According to Bayes’ theorem, what is the probability of Pr(B|A)?
```{r}
pa<-0.5
pb<-1/1000000  
pab<-0.5
pab*pb/pa
```
5.After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was “no statistical basis” for the expert’s claim. They expressed concern at the “misuse of statistics in the courts”. Eventually, Sally Clark was acquitted in June 2003. What did the expert miss?

a.He made an arithmetic error.
b.He made two mistakes. First, he misused the multiplication rule and did not take into account how rare it is for a mother to murder her children. After using Bayes’ rule, we found a probability closer to 0.5 than 1 in 73 million.
c.He mixed up the numerator and denominator of Bayes’ rule.
d.He did not use R.

b

6. Florida is one of the most closely watched states in the U.S. election because it has many electoral votes, and the election is generally close, and Florida tends to be a swing state that can vote either way. Create the following table with the polls taken during the last two weeks:
```{r}
library(tidyverse)
library(dslabs)
data(polls_us_election_2016)
polls <- polls_us_election_2016 %>% 
  filter(state == "Florida" & enddate >= "2016-11-04" ) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

Take the average spread of these polls. The CLT tells us this average is approximately normal. Calculate an average and provide an estimate of the standard error. Save your results in an object called results.

```{r}

results <- polls %>% summarize(avg = mean(spread),  se = sd(spread)/sqrt(n()))
results
```

7.Now assume a Bayesian model that sets the prior distribution for Florida’s election night spread d to be Normal with expected value μ and standard deviation τ. What are the interpretations of μ and τ?

a.μ and τ are arbitrary numbers that let us make probability statements about d.
b.μ and τ summarize what we would predict for Florida before seeing any polls. Based on past elections, we would set μ close to 0 because both Republicans and Democrats have won, and τ at about 0.02, because these elections tend to be close.
c.μ and τ summarize what we want to be true. We therefore set μ at 0.10 and τ at 0.01.
d.The choice of prior has no effect on Bayesian analysis.

b

8.The CLT tells us that our estimate of the spread d^ has normal distribution with expected value d and standard deviation σ calculated in problem 6. Use the formulas we showed for the posterior distribution to calculate the expected value of the posterior distribution if we set μ=0 and τ=0.01.

```{r}
sigma <- results$se
Y <- results$avg
tau <- 0.01
miu <- 0
B <- sigma^2 / (sigma^2 + tau^2)
B
```
```{r}
est <- miu + (1 - B) * (Y - miu)
est
```

9.Now compute the standard deviation of the posterior distribution.
```{r}
se <- sqrt(1/(1/sigma^2+1/tau^2))
se
```

10.Using the fact that the posterior distribution is normal, create an interval that has a 95% probability of occurring centered at the posterior expected value. Note that we call these credible intervals.
```{r}
ci <- c(est - qnorm(0.975) * se, est + qnorm(0.975) * se)
ci
```
11.According to this analysis, what was the probability that Trump wins Florida?

```{r}
pnorm(0, est, se)
```

12.Now use sapply function to change the prior variance from seq(0.05, 0.05, len = 100) and observe how the probability changes by making a plot.
```{r}
cal <- seq(0.005, 0.05, len = 100)
fun <- function(tau) {
  B <- sigma^2 / (sigma^2 + tau^2)
  est <- miu + (1 - B) * (Y - miu)
  se <- sqrt(1/(1/sigma^2+1/tau^2))
  pnorm(0, est, se)
}
pal <- sapply(cal, fun)
plot(cal, pal)
```

# 16.8 case study: election forecasting

```{r}
library(tidyverse)
library(dslabs)
polls <- polls_us_election_2016 %>% 
  filter(state == "U.S." & enddate >= "2016-10-31" &
           (grade %in% c("A+","A","A-","B+") | is.na(grade))) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

one_poll_per_pollster <- polls %>% group_by(pollster) %>% 
  filter(enddate == max(enddate)) %>%
  ungroup()

results <- one_poll_per_pollster %>% 
  summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>% 
  mutate(start = avg - 1.96*se, end = avg + 1.96*se) 
```

# 16.8.1 bayesian approach

we note that before any poll data is available, we can use data sources other than polling data. A popular approach is to use what pollsters call fundamentals, which are based on properties about the current economy that historically appear to have an effect in favor or against the incumbent party. We won’t use these here. Instead, we will use  μ=0 , which is interpreted as a model that simply does not provide any information on who will win. For the standard deviation, we will use recent historical data that shows the winner of the popular vote has an average spread of about 3.5%. Therefore, we set  τ=0.035 .

```{r}
mu <- 0
tau <- 0.035
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)

posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))

posterior_mean
posterior_se
```

```{r}
posterior_mean + c(-1.96, 1.96)*posterior_se
```

```{r}
1 - pnorm(0, posterior_mean, posterior_se)
```

# 16.8.2 the general bias

 An important observation that our model does not take into account is that it is common to see a general bias that affects many pollsters in the same way making the observed data correlated. There is no good explanation for this, but we do observe it in historical data: in one election, the average of polls favors Democrats by 2%, then in the following election they favor Republicans by 1%, then in the next election there is no bias, then in the following one Republicans are favored by 3%, and so on. In 2016, the polls were biased in favor of the Democrats by 1-2%.
 
# 16.8.3 mathematical representations of models

```{r}
set.seed(3)
J <- 6
N <- 2000
d <- .021
p <- (d + 1)/2
X <- d + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
```

```{r}
I <- 5
J <- 6
N <- 2000
X <- sapply(1:I, function(i){
  d + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
})
```

```{r}
I <- 5
J <- 6
N <- 2000
d <- .021
p <- (d + 1) / 2
h <- rnorm(I, 0, 0.025)
X <- sapply(1:I, function(i){
  d + h[i] + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
})
```

```{r}
sd(one_poll_per_pollster$spread)
```

```{r}
mu <- 0
tau <- 0.035
sigma <- sqrt(results$se^2 + .025^2)
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)

posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))

1 - pnorm(0, posterior_mean, posterior_se)
```

# 16.8.4 predicting the electoral college

```{r}
results <- polls_us_election_2016 %>%
  filter(state!="U.S." & 
           !str_detect(state, "CD") & 
           enddate >="2016-10-31" & 
           (grade %in% c("A+","A","A-","B+") | is.na(grade))) %>%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  group_by(state) %>%
  summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
  mutate(state = as.character(state))
```

```{r}
results %>% arrange(abs(avg))
```

```{r}
results <- left_join(results, results_us_election_2016, by = "state")
```

```{r}
results_us_election_2016 %>% filter(!state %in% results$state) %>% 
  pull(state)
```

```{r}
results <- results %>%
  mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))
```

```{r}
B <- 10000
mu <- 0
tau <- 0.02
clinton_EV <- replicate(B, {
  results %>% mutate(sigma = sd/sqrt(n), 
                   B = sigma^2 / (sigma^2 + tau^2),
                   posterior_mean = B * mu + (1 - B) * avg,
                   posterior_se = sqrt(1 / (1/sigma^2 + 1/tau^2)),
                   result = rnorm(length(posterior_mean), 
                                  posterior_mean, posterior_se),
                   clinton = ifelse(result > 0, electoral_votes, 0)) %>% 
    summarize(clinton = sum(clinton)) %>% 
    pull(clinton) + 7
})

mean(clinton_EV > 269)

```

```{r}
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
  results %>% mutate(sigma = sqrt(sd^2/n  + bias_sd^2),  
                   B = sigma^2 / (sigma^2 + tau^2),
                   posterior_mean = B*mu + (1-B)*avg,
                   posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)),
                   result = rnorm(length(posterior_mean), 
                                  posterior_mean, posterior_se),
                   clinton = ifelse(result>0, electoral_votes, 0)) %>% 
    summarize(clinton = sum(clinton) + 7) %>% 
    pull(clinton)
})
mean(clinton_EV_2 > 269)
```

# 16.8.5 forecasting

```{r}
one_pollster <- polls_us_election_2016 %>% 
  filter(pollster == "Ipsos" & state == "U.S.") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

```{r}
se <- one_pollster %>% 
  summarize(empirical = sd(spread), 
            theoretical = 2 * sqrt(mean(spread) * (1 - mean(spread)) /
                                     min(samplesize)))
se
```

# 16.9 exercises
1.

```{r}
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(state != "U.S." & enddate >= "2016-10-31") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```
Now for each poll use the CLT to create a 95% confidence interval for the spread reported by each poll. Call the resulting object cis with columns lower and upper for the limits of the confidence intervals. Use the select function to keep the columns state, startdate, end date, pollster, grade, spread, lower, upper.

```{r}
cis<-polls%>%
  mutate(x_hat=(spread+1)/2)%>%
  mutate(SE=2*sqrt(x_hat*(1-x_hat)/samplesize))%>%
  mutate(lower=spread-1.96*SE,upper=spread+1.96*SE)%>%
  select(state, startdate, enddate, pollster, grade, spread, lower, upper)
```
2.You can add the final result to the cis table you just created using the right_join function like this:

```{r}
add <- results_us_election_2016 %>% 
  mutate(actual_spread = clinton/100 - trump/100) %>% 
  select(state, actual_spread)
cis <- cis %>% 
  mutate(state = as.character(state)) %>% 
  left_join(add, by = "state")
```
Now determine how often the 95% confidence interval includes the actual result.

```{r}
cis%>%mutate(len=lower<=actual_spread & upper>=actual_spread)%>%
  summarize(interval=mean(len))
```

3.Repeat this, but show the proportion of hits for each pollster. Show only pollsters with more than 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster. Hint: use n=n(), grade = grade[1] in the call to summarize.

```{r}
cis%>%
  mutate(len=lower<=actual_spread & upper>=actual_spread)%>% 
  group_by(pollster)%>%
  filter(n()>=5)%>%
  summarize(interval=mean(len), n=n(), grade=grade[1])%>%
  arrange(desc(interval))
```
4. Repeat exercise 3, but instead of pollster, stratify by state. Note that here we can’t show grades.

```{r}
bystate<-cis%>%
  mutate(len=lower<=actual_spread & upper>=actual_spread)%>% 
  group_by(state)%>%
  filter(n()>=5)%>%
  summarize(interval=mean(len), n=n())%>%
  arrange(desc(interval))
```
5.Make a barplot based on the result of exercise 4. Use coord_flip.

```{r}
bystate%>%ggplot(aes(state, interval))+geom_bar(stat='identity')+coord_flip()
```
6. Add two columns to the cis table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same. Hint: use the function sign. Call the object resids.

```{r}
resids <- cis %>% mutate(error = spread - actual_spread, hit = sign(spread) == sign(actual_spread))
```

7. Create a plot like in exercise 5, but for the proportion of times the sign of the spread agreed.
```{r}
resids %>%  group_by(state) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n()) %>%
  mutate(state = reorder(state, proportion_hits)) %>%
  ggplot(aes(state, proportion_hits)) + 
  geom_bar(stat = "identity") +
 coord_flip()
```
8. In exercise 7, we see that for most states the polls had it right 100% of the time. For only 9 states did the polls miss more than 25% of the time. In particular, notice that in Wisconsin every single poll got it wrong. In Pennsylvania and Michigan more than 90% of the polls had the signs wrong. Make a histogram of the errors. What is the median of these errors?

```{r}
resids%>%ggplot(aes(error))+geom_histogram()
```
```{r}
resids%>%summarize(median=median(error))
```
9.We see that at the state level, the median error was 3% in favor of Clinton. The distribution is not centered at 0, but at 0.03. This is the general bias we described in the section above. Create a boxplot to see if the bias was general to all states or it affected some states differently. Use filter(grade %in% c("A+","A","A-","B+") | is.na(grade))) to only include pollsters with high grades.

```{r}
resids %>% filter(grade %in% c("A+","A","A-","B+") | is.na(grade)) %>% mutate(state = reorder(state, error)) %>% ggplot(aes(state, error)) + geom_boxplot() 
```

10.Some of these states only have a few polls. Repeat exercise 9, but only include states with 5 good polls or more. Hint: use group_by, filter then ungroup. You will see that the West (Washington, New Mexico, California) underestimated Hillary’s performance, while the Midwest (Michigan, Pennsylvania, Wisconsin, Ohio, Missouri) overestimated it. In our simulation, we did not model this behavior since we added general bias, rather than a regional bias. Note that some pollsters may now be modeling correlation between similar states and estimating this correlation from historical data. To learn more about this, you can learn about random effects and mixed models.

```{r}
resids %>% filter(grade %in% c("A+","A","A-","B+") | is.na(grade)) %>% group_by(state) %>% filter(n() >= 5) %>% ungroup() %>% mutate(state = reorder(state, error)) %>% ggplot(aes(state, error)) +  geom_boxplot() 
```


# 16.10 the t-distribution

```{r}
one_poll_per_pollster %>%
  ggplot(aes(sample=spread)) + stat_qq()
```

```{r}
z <- qt(0.975,  nrow(one_poll_per_pollster)-1)
one_poll_per_pollster %>% 
  summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) %>% 
  mutate(start = avg - moe, end = avg + moe) 
```

```{r}
qt(0.975, 14)
```

```{r}
qnorm(0.975)
```

```{r}
data("polls_us_election_2016")
polls_us_election_2016 %>%
  filter(state =="Wisconsin" &
           enddate >="2016-10-31" & 
           (grade %in% c("A+","A","A-","B+") | is.na(grade))) %>%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  mutate(state = as.character(state)) %>%
  left_join(results_us_election_2016, by = "state") %>%
  mutate(actual = clinton/100 - trump/100) %>%
  summarize(actual = first(actual), avg = mean(spread), 
            sd = sd(spread), n = n()) %>%
  select(actual, avg, sd, n)
```












Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
